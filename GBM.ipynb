{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c76d34",
   "metadata": {},
   "source": [
    "![title](gbm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c18cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe51224",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.multivariate_normal([2,-5], [[4, -1.6], [-1.6, 6]], size=100)\n",
    "y = X @[5,1] + np.random.normal(scale=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e99d687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 2), (100,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "110135f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e51ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_0 = np.full(y_train.shape, y_train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3b88f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(y_true, y_pred):\n",
    "    return - (y_true - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "809a186b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 80.98669029032435\n",
      "Loss: 31.49500829435127\n",
      "Loss: 22.012730703948872\n",
      "Loss: 16.48483855245954\n",
      "Loss: 13.732238370684206\n",
      "Loss: 11.164831618267288\n",
      "Loss: 9.455156024535095\n",
      "Loss: 7.805963238697475\n",
      "Loss: 7.101964181685123\n",
      "Loss: 6.0111012315324475\n",
      "Loss: 5.512082700385349\n",
      "Loss: 4.876018064410004\n",
      "Loss: 4.45909052733838\n",
      "Loss: 4.032618627360535\n",
      "Loss: 3.7402117809433304\n",
      "Loss: 3.469520195706132\n",
      "Loss: 3.1198285065213867\n",
      "Loss: 2.9414373303599586\n",
      "Loss: 2.79612996273168\n",
      "Loss: 2.5908741572668936\n",
      "Loss: 2.4766411668548205\n",
      "Loss: 2.361673520062467\n",
      "Loss: 2.219176718249973\n",
      "Loss: 2.051660955742931\n",
      "Loss: 1.9348546230806123\n",
      "Loss: 1.8361790683119352\n",
      "Loss: 1.7346255125847299\n",
      "Loss: 1.6678505718325953\n",
      "Loss: 1.6200512716278481\n",
      "Loss: 1.5026882902005112\n",
      "Loss: 1.4166945365867292\n",
      "Loss: 1.3317745340734735\n",
      "Loss: 1.2907549697973846\n",
      "Loss: 1.2291980041806576\n",
      "Loss: 1.172692166132052\n",
      "Loss: 1.123307346174484\n",
      "Loss: 1.0763578194206982\n",
      "Loss: 0.9953675197361035\n",
      "Loss: 0.9564898588386321\n",
      "Loss: 0.9099437573747938\n",
      "Loss: 0.8685327374099662\n",
      "Loss: 0.820556096706951\n",
      "Loss: 0.7883169649190473\n",
      "Loss: 0.7548248446598135\n",
      "Loss: 0.7235467904365656\n",
      "Loss: 0.6986368695808033\n",
      "Loss: 0.6720877653508978\n",
      "Loss: 0.6394823360598504\n",
      "Loss: 0.6061950484935467\n",
      "Loss: 0.5858588377033547\n",
      "Loss: 0.561360992540892\n",
      "Loss: 0.5475962266771863\n",
      "Loss: 0.5281125147402355\n",
      "Loss: 0.5147060275143335\n",
      "Loss: 0.49780462369468137\n",
      "Loss: 0.48492856729709605\n",
      "Loss: 0.4702625515310127\n",
      "Loss: 0.45543554724703245\n",
      "Loss: 0.435161568291952\n",
      "Loss: 0.4227346854726763\n",
      "Loss: 0.41170216903778706\n",
      "Loss: 0.4033945878561588\n",
      "Loss: 0.39018022446347894\n",
      "Loss: 0.3774083892986203\n",
      "Loss: 0.36329735802220836\n",
      "Loss: 0.3527716236391939\n",
      "Loss: 0.34336083048915184\n",
      "Loss: 0.3318153831724659\n",
      "Loss: 0.3259530808799483\n",
      "Loss: 0.3183777288395683\n",
      "Loss: 0.31160009415525364\n",
      "Loss: 0.30612342592482533\n",
      "Loss: 0.3005584710805734\n",
      "Loss: 0.29433219011201817\n",
      "Loss: 0.28948282130514663\n",
      "Loss: 0.27872723782660025\n",
      "Loss: 0.2716550733475562\n",
      "Loss: 0.2646537238333572\n",
      "Loss: 0.25914403014159426\n",
      "Loss: 0.25302958318769914\n",
      "Loss: 0.24841259842546287\n",
      "Loss: 0.2434719366206256\n",
      "Loss: 0.23912981345645876\n",
      "Loss: 0.2327746684298156\n",
      "Loss: 0.22948536873008224\n",
      "Loss: 0.2238929722257205\n",
      "Loss: 0.21850248076301065\n",
      "Loss: 0.21285800166947813\n",
      "Loss: 0.20866529139480444\n",
      "Loss: 0.20463006333779046\n",
      "Loss: 0.1991996195172114\n",
      "Loss: 0.19494505304224685\n",
      "Loss: 0.190876583293313\n",
      "Loss: 0.18755376556195186\n",
      "Loss: 0.18420014011186822\n",
      "Loss: 0.1812815488501959\n",
      "Loss: 0.17846732916219604\n",
      "Loss: 0.17342373222378415\n",
      "Loss: 0.16938843323754108\n",
      "Loss: 0.16623916647834763\n",
      "Loss: 0.1643723053098698\n",
      "Loss: 0.16163969939398465\n",
      "Loss: 0.15900941208571506\n",
      "Loss: 0.15688703983823682\n",
      "Loss: 0.154002805642095\n",
      "Loss: 0.15168873264591196\n",
      "Loss: 0.14863136524082238\n",
      "Loss: 0.14631344132974974\n",
      "Loss: 0.144111125465764\n",
      "Loss: 0.14106762731519834\n",
      "Loss: 0.13871006436967503\n",
      "Loss: 0.13714515339504088\n",
      "Loss: 0.13496071130647624\n",
      "Loss: 0.13293404356009886\n",
      "Loss: 0.1302101261877474\n",
      "Loss: 0.12753050964678248\n",
      "Loss: 0.12566854265990282\n",
      "Loss: 0.12414189173690761\n",
      "Loss: 0.12200967109488317\n",
      "Loss: 0.12025755968752484\n",
      "Loss: 0.11816387304546509\n",
      "Loss: 0.11684225088011144\n",
      "Loss: 0.11481229112011221\n",
      "Loss: 0.11270382592294759\n",
      "Loss: 0.11136410004398817\n",
      "Loss: 0.10978877393928997\n",
      "Loss: 0.10793972145779586\n",
      "Loss: 0.1063777889268441\n",
      "Loss: 0.10505839250318302\n",
      "Loss: 0.10352774898795293\n",
      "Loss: 0.10156134457845291\n",
      "Loss: 0.09943854046998862\n",
      "Loss: 0.09782116591115886\n",
      "Loss: 0.09632687274625897\n",
      "Loss: 0.09486227642837171\n",
      "Loss: 0.09352124374601782\n",
      "Loss: 0.09244032516153432\n",
      "Loss: 0.09123815888249803\n",
      "Loss: 0.0904007341536813\n",
      "Loss: 0.08961263462115145\n",
      "Loss: 0.08829946326799205\n",
      "Loss: 0.08726216824952993\n",
      "Loss: 0.0861182278211089\n",
      "Loss: 0.08517467035312318\n",
      "Loss: 0.08412443673555293\n",
      "Loss: 0.08328828340059276\n",
      "Loss: 0.08209631798606055\n",
      "Loss: 0.0812400124191747\n",
      "Loss: 0.08010189602894657\n",
      "Loss: 0.07925690714306956\n",
      "Loss: 0.07859484588497673\n",
      "Loss: 0.07772613694933048\n",
      "Loss: 0.07688517829233431\n",
      "Loss: 0.0760791158522833\n",
      "Loss: 0.07526872503250615\n",
      "Loss: 0.07435633970220208\n",
      "Loss: 0.07373008165000837\n",
      "Loss: 0.07281240444937012\n",
      "Loss: 0.07137372101327615\n",
      "Loss: 0.07023084727083008\n",
      "Loss: 0.0694015669333233\n",
      "Loss: 0.06846934951989385\n",
      "Loss: 0.06779768976860297\n",
      "Loss: 0.06703862876027951\n",
      "Loss: 0.06633831021160158\n",
      "Loss: 0.06554455725632967\n",
      "Loss: 0.06505172544674954\n",
      "Loss: 0.06419200550330674\n",
      "Loss: 0.06338286076911903\n",
      "Loss: 0.0626242060532871\n",
      "Loss: 0.062090906229640884\n",
      "Loss: 0.06130515522516331\n",
      "Loss: 0.060616531021030935\n",
      "Loss: 0.059833184137605674\n",
      "Loss: 0.05924183403933365\n",
      "Loss: 0.058662578901750105\n",
      "Loss: 0.05809554361421731\n",
      "Loss: 0.057384421329369224\n",
      "Loss: 0.056803721913566654\n",
      "Loss: 0.05627910137678473\n",
      "Loss: 0.05555718195039695\n",
      "Loss: 0.05511435715038713\n",
      "Loss: 0.054574814208361674\n",
      "Loss: 0.054103192372531875\n",
      "Loss: 0.05355460366858628\n",
      "Loss: 0.053102789405133195\n",
      "Loss: 0.05265975646004879\n",
      "Loss: 0.0520867257255904\n",
      "Loss: 0.051615134306479064\n",
      "Loss: 0.0510420394066587\n",
      "Loss: 0.050604519253669045\n",
      "Loss: 0.05020724092749021\n",
      "Loss: 0.04969587768205955\n",
      "Loss: 0.04917220951400328\n",
      "Loss: 0.048729856278875064\n",
      "Loss: 0.048218239426140286\n",
      "Loss: 0.04777792659089053\n",
      "Loss: 0.04735931623393941\n",
      "Loss: 0.04700055103898884\n",
      "Loss: 0.046657364438868\n",
      "Loss: 0.046357419759981644\n",
      "Loss: 0.04593605749262385\n",
      "Loss: 0.045463505217657484\n",
      "Loss: 0.04498699401503086\n",
      "Loss: 0.04448036551208488\n",
      "Loss: 0.04399840324229223\n",
      "Loss: 0.043673672043151805\n",
      "Loss: 0.04333096616118137\n",
      "Loss: 0.042935704257647506\n",
      "Loss: 0.04255133087059461\n",
      "Loss: 0.04228897714562784\n",
      "Loss: 0.042006033349949734\n",
      "Loss: 0.04173848349944213\n",
      "Loss: 0.04130350572207235\n",
      "Loss: 0.040831153297961414\n",
      "Loss: 0.040340338503263996\n",
      "Loss: 0.03996089761895237\n",
      "Loss: 0.03966247140209127\n",
      "Loss: 0.039317800661129464\n",
      "Loss: 0.03902591979998471\n",
      "Loss: 0.03860902042115388\n",
      "Loss: 0.038351317771028993\n",
      "Loss: 0.03794637315830016\n",
      "Loss: 0.03765178946055551\n",
      "Loss: 0.037343541309276224\n",
      "Loss: 0.0370562224433686\n",
      "Loss: 0.03679272871184544\n",
      "Loss: 0.03651830032884889\n",
      "Loss: 0.036198071050574425\n",
      "Loss: 0.03582857199238452\n",
      "Loss: 0.035375367082018744\n",
      "Loss: 0.03503139824596779\n",
      "Loss: 0.0347960577614881\n",
      "Loss: 0.034481830441593135\n",
      "Loss: 0.03421790497459592\n",
      "Loss: 0.03386512931998402\n",
      "Loss: 0.03351707562665954\n",
      "Loss: 0.03327139066666584\n",
      "Loss: 0.0330166082494459\n",
      "Loss: 0.03275993866264474\n",
      "Loss: 0.032577098876482205\n",
      "Loss: 0.032321704078806905\n",
      "Loss: 0.03207616556582561\n",
      "Loss: 0.03177024714166406\n",
      "Loss: 0.031528597294007144\n",
      "Loss: 0.031335590548613027\n",
      "Loss: 0.03112110573693178\n",
      "Loss: 0.030941348184252378\n",
      "Loss: 0.030758175047310654\n",
      "Loss: 0.030614049285297876\n",
      "Loss: 0.030383945183703933\n",
      "Loss: 0.03011830957355072\n",
      "Loss: 0.029832736109924873\n",
      "Loss: 0.029591506351830877\n",
      "Loss: 0.02937330567206227\n",
      "Loss: 0.02919406939939508\n",
      "Loss: 0.028990297678272256\n",
      "Loss: 0.028809104376325247\n",
      "Loss: 0.02861898665661074\n",
      "Loss: 0.028413003125422994\n",
      "Loss: 0.028248818535968383\n",
      "Loss: 0.02801347471633987\n",
      "Loss: 0.027848916168896693\n",
      "Loss: 0.027623247928943755\n",
      "Loss: 0.027485636215318916\n",
      "Loss: 0.027311680434003442\n",
      "Loss: 0.02715017919143128\n",
      "Loss: 0.027005046318038843\n",
      "Loss: 0.026856105169591048\n",
      "Loss: 0.0266756004462295\n",
      "Loss: 0.026504847403288435\n",
      "Loss: 0.026293181823599746\n",
      "Loss: 0.02610218671067758\n",
      "Loss: 0.02592984346425168\n",
      "Loss: 0.02576441930641913\n",
      "Loss: 0.025617063104269106\n",
      "Loss: 0.02548644516964727\n",
      "Loss: 0.025353980556523225\n",
      "Loss: 0.025223406426618352\n",
      "Loss: 0.02508982771323904\n",
      "Loss: 0.024874271972967855\n",
      "Loss: 0.024688563150964328\n",
      "Loss: 0.024450848572521194\n",
      "Loss: 0.024242700483934204\n",
      "Loss: 0.024060441461091833\n",
      "Loss: 0.0238778990371255\n",
      "Loss: 0.023733431344676134\n",
      "Loss: 0.023573535320665775\n",
      "Loss: 0.023393857885552087\n",
      "Loss: 0.023200305383270642\n",
      "Loss: 0.023004125300784665\n",
      "Loss: 0.02283173780008863\n",
      "Loss: 0.02267928100561235\n",
      "Loss: 0.022522464843087002\n",
      "Loss: 0.022364509629546274\n",
      "Loss: 0.02222884996502258\n",
      "Loss: 0.0220809549210266\n",
      "Loss: 0.02195311194080978\n",
      "Loss: 0.021812871492055283\n",
      "Loss: 0.021699784993023597\n",
      "Loss: 0.021589793953672165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9997334155294373"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit\n",
    "\n",
    "M = 300\n",
    "depth=1\n",
    "pred = pred_0\n",
    "trees = []\n",
    "print('Loss:', mean_squared_error(y_train, pred))\n",
    "\n",
    "\n",
    "for m in range(M):\n",
    "    dt = DecisionTreeRegressor(max_depth=depth)\n",
    "    \n",
    "    r = - grad(y_train, pred)\n",
    "    dt.fit(X_train, r)\n",
    "\n",
    "    trees.append(dt)\n",
    "\n",
    "    pred = pred + dt.predict(X_train)\n",
    "    print('Loss:', mean_squared_error(y_train, pred))\n",
    "    \n",
    "\n",
    "r2_score(y_train, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7f685934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "\n",
    "y_pred = pred_0[0] + np.array([tree.predict(X_test) for tree in trees]).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0b7d035a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9784699174350129"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212288e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb8608e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
